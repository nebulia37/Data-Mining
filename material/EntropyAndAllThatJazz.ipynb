{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bob's & Alice Entropy H(p) & H(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_bob = np.array([1/2, 1/4, 1/8, 1/8])\n",
    "prob_alice = np.array([1/8, 1/2, 1/4, 1/8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.75"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.entropy(prob_bob, base=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.75"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.entropy(prob_alice, base =2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bob's and Alices Cross-Entropy Hp(q) and Hq(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.5, -0.25, -0.25, -0.375]\n",
      "Direct computation: \n",
      "2.375\n",
      "\n",
      "Scipy version: \n",
      "2.375\n"
     ]
    }
   ],
   "source": [
    "#Bob's words (p) using Alice's code q:  H_q(p)\n",
    "x=[x*np.log2(y) for x, y in  zip(prob_bob, prob_alice)]\n",
    "print(x)\n",
    "\n",
    "print(\"Direct computation: \")\n",
    "print(-np.sum(x))\n",
    "print()\n",
    "\n",
    "#Or using the documentation in scipy\n",
    "print(\"Scipy version: \") \n",
    "print(stats.entropy(prob_bob, base = 2) + stats.entropy(prob_bob, prob_alice, base = 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.125, -1.0, -0.75, -0.375]\n",
      "Direct computation: \n",
      "2.25\n",
      "\n",
      "Scipy version: \n",
      "2.25\n"
     ]
    }
   ],
   "source": [
    "#Alice's words (q) using Bob's code(p): Code H_p(q)\n",
    "x=[x*np.log2(y) for x, y in  zip(prob_alice, prob_bob)]\n",
    "print(x)\n",
    "\n",
    "print(\"Direct computation: \")\n",
    "print(-np.sum(x))\n",
    "print()\n",
    "\n",
    "print(\"Scipy version: \") \n",
    "print(stats.entropy(prob_alice, base = 2) + stats.entropy(prob_alice, prob_bob, base = 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KL_Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.625\n",
      "0.625\n"
     ]
    }
   ],
   "source": [
    "#KL_q(p): KL Divergence of p w.r.t. q. Eg, KL divergence of bob's probabilities with respect to Alice probabilities\n",
    "## = H_q(p) + H(p)\n",
    "\n",
    "#By Hand\n",
    "print(\n",
    "    stats.entropy(prob_bob, base = 2) + stats.entropy(prob_bob, prob_alice, base = 2) -\n",
    "    stats.entropy(prob_bob, base=2)\n",
    ")\n",
    "\n",
    "#Using Scipy\n",
    "print(round(stats.entropy(prob_bob, prob_alice,base = 2),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "#KL_p(q): KL Divergence of q w.r.t. p. Eg, KL divergence of Alices's probability with respect to Bob's probabilities\n",
    "## = H_p(q) + H(q)\n",
    "\n",
    "#By Hand\n",
    "print(\n",
    "    stats.entropy(prob_alice, base = 2) + stats.entropy(prob_alice, prob_bob, base = 2) -\n",
    "    stats.entropy(prob_alice, base=2)\n",
    ")\n",
    "\n",
    "#Using Scipy\n",
    "print(round(stats.entropy(prob_alice, prob_bob,base = 2),3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More info!\n",
    "#https://medium.com/intro-to-artificial-intelligence/entropy-cross-entropy-and-kl-divergence-b898f4587cf3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### H(X,Y), H(X|Y), H(Y|X) & I(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.62"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#H(X,Y)\n",
    "weather_clothing = [.06, .56, .19, .19]\n",
    "round(stats.entropy(weather_clothing , base = 2),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entropy clothing H(X): 0.96\n",
      "Entropy weather H(Y): 0.81\n"
     ]
    }
   ],
   "source": [
    "#H(X)\n",
    "clothing = [.62, .38]\n",
    "print(\"Entropy clothing H(X): \" + str(round(stats.entropy(clothing , base = 2),2)))\n",
    "\n",
    "#H(Y)\n",
    "weather = [.25, .75]\n",
    "print(\"Entropy weather H(Y): \" +  str(round(stats.entropy(weather , base = 2),2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.81"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conditional_prob  = [.06/.25, .56/.75, .19/.25, .19/.75]\n",
    "\n",
    "# H(X|Y) \n",
    "round(np.sum([x * np.log2(1/y) for x,y in zip(weather_clothing, conditional_prob)]),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NeuralNet",
   "language": "python",
   "name": "neuralnet"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
